{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import logging\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.DEBUG)\n",
    "import itertools\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_probability as tfp\n",
    "import nngp1\n",
    "import gpr\n",
    "import ard\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfk = tfp.math.psd_kernels\n",
    "%pylab inline\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    project_dir = os.getcwd()\n",
    "    data_dir = os.getcwd()+'/data'\n",
    "    model_type = 'bert'\n",
    "    model_name_or_path = 'bert-base-cased' #can be path of fined tuned model    \n",
    "    task_name = 'asap'\n",
    "    output_dir = 'results/'+task_name\n",
    "    item_id = 1\n",
    "\n",
    "args=args\n",
    "\n",
    "asap_ranges = {\n",
    "    1: (2,12),\n",
    "    2: (1,6),\n",
    "    3: (0,3),\n",
    "    4: (0,3),\n",
    "    5: (0,4),\n",
    "    6: (0,4),\n",
    "    7: (0,30),\n",
    "    8: (0,60)\n",
    "}\n",
    "\n",
    "wide_features = ['unique_word_count', 'sentence_length_entropy', 'sentence_length_words_mean', \n",
    "                 'sentence_count', 'type_token_ratio', 'word_lengths_percentile_75', 'syllable_ratio', \n",
    "                 'sentence_diversity', 'unique_spelling_error_ratio']\n",
    "\n",
    "df_tr = pd.read_csv(args.data_dir+'/essay_asap{}_tr.csv'.format(args.item_id))\n",
    "X_w_train = df_tr[wide_features]\n",
    "labels_tr = (df_tr['dimscore1']-asap_ranges[args.item_id][0])/(asap_ranges[args.item_id][1]-asap_ranges[args.item_id][0])\n",
    "n_train = X_w_train.shape[0]\n",
    "\n",
    "df_dev = pd.read_csv(args.data_dir+'/essay_asap{}_te.csv'.format(args.item_id))\n",
    "X_w_dev = df_dev[wide_features]\n",
    "labels_dev = df_dev['dimscore1']\n",
    "n_dev = X_w_dev.shape[0]\n",
    "n_wide_features = len(wide_features)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_w_std = scaler.fit_transform(np.concatenate([X_w_train, X_w_dev], axis=0))\n",
    "X_w_train_std = X_w_std[0:n_train, :]\n",
    "X_w_dev_std = X_w_std[n_train:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optmizing wide kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_w_std = scaler.fit_transform(np.concatenate([X_w_train, X_w_dev], axis=0))\n",
    "X_w_train_std = X_w_std[0:1291, :]\n",
    "X_w_dev_std = X_w_std[1291:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_w_train_std\n",
    "X_dev = X_w_dev_std\n",
    "#labels_tr = tf.constant((np.array(labels_tr.tolist())-2.)/10., dtype=tf.float64)\n",
    "#labels_dev = np.array(labels_dev.tolist())\n",
    "\n",
    "conf_priors = np.array([[2.]*n_wide_features, [0.75]*n_wide_features])\n",
    "\n",
    "amplitude = (np.finfo(np.float64).tiny +\n",
    "           tf.nn.softplus(\n",
    "               tf.Variable(10.*tf.ones(1, dtype=tf.float64)),\n",
    "               name='amplitude'))\n",
    "\n",
    "length_scale = (np.finfo(np.float64).tiny +\n",
    "              tf.nn.softplus(\n",
    "                  tf.Variable([30.]*n_wide_features, dtype=np.float64),\n",
    "                  name='length_scale'))\n",
    "\n",
    "rv_scale = tfd.Gamma(\n",
    "  concentration=conf_priors[0],\n",
    "  rate=conf_priors[1],\n",
    "  name='length_scale_prior')\n",
    "\n",
    "kernel = ard.InputScaledKernel(tfk.ExponentiatedQuadratic(amplitude),\n",
    "                         length_scale,\n",
    "                         name='ARD_kernel')\n",
    "\n",
    "gp = tfd.Independent(\n",
    "  tfd.GaussianProcess(kernel=kernel, index_points=X),\n",
    "  reinterpreted_batch_ndims=1)\n",
    "\n",
    "# Joint log prob of length_scale params and data\n",
    "log_likelihood = (gp.log_prob(tf.constant(labels_tr.tolist(), dtype=tf.float64)) +\n",
    "                tf.reduce_sum(rv_scale.log_prob(length_scale)))\n",
    "\n",
    "# Optimization target (neg log likelihood) and optimizer. Use a fairly\n",
    "# generous learning rate (adaptive optimizer will adapt :))\n",
    "loss = -log_likelihood\n",
    "opt = tf.train.AdamOptimizer(.05).minimize(loss)\n",
    "\n",
    "num_iters = 400\n",
    "losses_ = np.zeros(num_iters, np.float32)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_iters):\n",
    "        _, losses_[i] = sess.run([opt, loss])\n",
    "#         if losses_[i] < 1e-10:\n",
    "#             break\n",
    "    \n",
    "    length_scale_opt = length_scale.eval()\n",
    "    amplitude_opt = amplitude.eval()\n",
    "\n",
    "    plt.plot(losses_)\n",
    "    k_w_data_data = sess.run(kernel.matrix(X, X))\n",
    "    k_w_data_test = sess.run(kernel.matrix(X, X_dev))\n",
    "    k_w_test_test = sess.run(kernel.matrix(X_dev, X_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing deep kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d_train = np.load(args.data_dir+'/X_d_train_{}.npy'.format(args.item_id))\n",
    "X_d_dev = np.load(args.data_dir+'/X_d_test_{}.npy'.format(args.item_id))\n",
    "\n",
    "config = {'depth': 8, 'nonfn': ['relu'], 'bv': 29.213, 'wv': 7.213}\n",
    "\n",
    "var_aa_grid = np.load(args.project_dir+'/grid_data/var_aa_grid_{}.npy'.format(config['nonfn'][0]))\n",
    "corr_ab_grid = np.load(args.project_dir+'/grid_data/corr_ab_grid_{}.npy'.format(config['nonfn'][0]))\n",
    "qaa_grid = np.load(args.project_dir+'/grid_data/qaa_grid_{}.npy'.format(config['nonfn'][0]))\n",
    "qab_grid = np.load(args.project_dir+'/grid_data/qab_grid_{}.npy'.format(config['nonfn'][0]))\n",
    "\n",
    "var_aa_grid = tf.convert_to_tensor(var_aa_grid)\n",
    "corr_ab_grid = tf.convert_to_tensor(corr_ab_grid)\n",
    "qaa_grid = tf.convert_to_tensor(qaa_grid)\n",
    "qab_grid = tf.convert_to_tensor(qab_grid)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    nngp_kernel = nngp1.NNGPKernel(\n",
    "        depth=config['depth'],\n",
    "        weight_var=config['wv'],\n",
    "        bias_var=config['bv'],\n",
    "        nonlin_fn=config['nonfn'],\n",
    "        grid_path=args.project_dir+'/grid_data/',\n",
    "        var_aa_grid=var_aa_grid,\n",
    "    corr_ab_grid=corr_ab_grid,\n",
    "    qaa_grid=qaa_grid,\n",
    "    qab_grid=qab_grid,\n",
    "        use_fixed_point_norm=False)\n",
    "    \n",
    "    X_d_train_pl = tf.placeholder(dtype=tf.float64, shape=[X_d_train.shape[0], X_d_train.shape[1]])\n",
    "    X_d_dev_pl = tf.placeholder(dtype=tf.float64, shape=[X_d_dev.shape[0], X_d_dev.shape[1]])\n",
    "    #X_w_train_pl = tf.placeholder(dtype=tf.float64, shape=[X_w_train.shape[0], X_w_train.shape[1]])\n",
    "    #X_w_dev_pl = tf.placeholder(dtype=tf.float64, shape=[X_w_dev.shape[0], X_w_dev.shape[1]])\n",
    "\n",
    "    kdiag = nngp_kernel.k_diag(input_x=X_d_train_pl, input_wx=None)\n",
    "    k_data_data = nngp_kernel.k_full(input1=X_d_train_pl, w_input1=None)\n",
    "    k_data_test = tf.identity(nngp_kernel.k_full(input1=X_d_train_pl, input2=X_d_dev_pl, w_input1=None, w_input2=None))\n",
    "    k_test_test = nngp_kernel.k_full(input1=X_d_dev_pl, w_input1=None)\n",
    "\n",
    "    feed_dict={X_d_train_pl: X_d_train, X_d_dev_pl:X_d_dev}\n",
    "    kdiag = sess.run(kdiag, feed_dict={X_d_train_pl: X_d_train})\n",
    "    k_data_data, k_data_test, k_test_test = sess.run([k_data_data, k_data_test, k_test_test], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K_data_data = k_data_data+k_w_data_data\n",
    "K_data_test = k_data_test+k_w_data_test\n",
    "K_test_test = k_test_test+k_w_test_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpr import GaussianProcessRegression\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = GaussianProcessRegression(K_data_data[0], K_data_test[0], np.reshape(labels_tr.tolist(), (-1,1)))\n",
    "    gp_pred, var_pred =  model.predict(K_test_test[0], sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "gp_pred_int = np.rint(gp_pred*10+2)\n",
    "qwk = cohen_kappa_score(np.array(labels_dev, dtype=float), gp_pred_int, weights = 'quadratic')\n",
    "print(qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JupyterPy2",
   "language": "python",
   "name": "ipykernel_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
